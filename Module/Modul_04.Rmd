---
title: "Module 04: There is something between us"
output: 
  learnr::tutorial:
    progressive: true
    css: "css/style.css"
runtime: shiny_prerendered
---

<a href="https://ki-campus.org/">
<img border="0" alt="KICampusLogo" src="images/KIcampusLogo.png" width="100" height="30" style="float: right">
</a>

```{r setup, include=FALSE}
library(learnr)
library(knitr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(emojifont)

theme.fom <- theme_classic(22*1.04)
theme.fom <- theme.fom
theme_set(
  theme.fom  
)

library(ggdag)
# DAG
co <- data.frame(x=c(0,1,2), y=c(0,0,0), name=c("X", "Z", "Y"))
DAG_Chain <- dagify(Z ~ X,
                    Y ~ Z,
                   coords = co) %>% 
  ggdag() + 
  geom_dag_point(colour = c("#0F710B", "#0000FF","#DA70D6")) + 
  geom_dag_text(size = 8) +
  theme_dag_blank() +
  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(15, "pt"), type = "closed")) +
  geom_text(label = "X - Learning\nZ - Knowledge\nY - Understanding", 
            hjust = 1, vjust = 2,
            x = 2, y = 0, size = 7, color = "darkgrey") 


library(mosaic)

U_X <- function(n = 1) runif(n, min = 1, max = 10)
f_Z <- function(x) 5*x + rnorm(length(x))
f_Y <- function(z) 3*z + rnorm(length(z))

# Daten und Funktion
set.seed(1992)
n <- 100
SimData <- tibble(x = U_X(n)) %>%
  mutate(z = f_Z(x)) %>%
  mutate(y = f_Y(z))

# Ergebnisse
ModelA <- lm(y ~ x, data = SimData)
```

## Learning objectives

In this module you will learn:

- about causal chains,

- mediators, and

- that sometimes it is better not to consider certain variables in the analysis.

## One thing leads to another

Even complex causal graphs consist of simple basic elements. One of them is the so-called **chain**. 

As a reminder, causality flows along the arrows. $$A \rightarrow B$$ states that $B$ *listens* to $A$. For example, if it rains, ($A$), the road gets wet ($B$). 

In a chain, we simply add a third variable: $$A \rightarrow B \rightarrow C.$$ 
For example: if it rains, ($A$) the road gets wet ($B$) and thus it may turn slippery ($C$).


## Learning and understanding

The following example is fictitious &ndash; and a very strong simplification. Moreover, the important question of how the variables are measured in each case will be ignored.

***

*Note*: The AI Campus course [Stadt | Land | DatenFluss](https://ki-campus.org/datenfluss) has a section on this topic,  "Welche Information steckt in Daten?"

***

Suppose <green>learning</green> <nobr>($\color{green}{X}$)</nobr> leads to <violet>knowledge</violet> <nobr>($\color{violet}{Z}$)</nobr>, that is, by learning you acquire knowledge. Furthermore, <violet>knowledge</violet> <nobr>($\color{violet}{Z}$)</nobr> leads to <blue>understanding</blue> <nobr>($\color{blue}{Y}$)</nobr>, i.e., through your knowledge you come to an understanding.

If this highly simplified model is correct, then the assumption can be represented in a causal graph:

```{r DAG_Chain, echo=FALSE, fig.align='center', out.width='85%'}
plot(DAG_Chain)
```

##

The structural causal model consists of the following assignments:

\begin{eqnarray*}
\color{green}{X} &=& U_{\color{green}{X}}\\
\color{violet}{Z} &=& f_{\color{violet}{Z}}(\color{green}{X}, U_{\color{violet}{Z}})\\
\color{blue}{Y} &=& f_{\color{blue}{Y}}(\color{violet}{Z},U_{\color{blue}{Y}}).
\end{eqnarray*}

The value of <green>learning</green> ($\color{green}{X}$) is determined outside of the model ($U_{\color{green}{X}}$). 
The value of <violet>knowledge</violet> ($\color{violet}{Z}$) depends on the value of <green>learning</green> ($\color{green}{X}$) &ndash; and other factors ($U_{\color{violet}{Z}}$).
Ultimately, <blue>understanding</blue> ($\color{blue}{Y}$) depends on <violet>knowledge</violet> ($\color{violet}{Z}$) &ndash; and $U_{\color{blue}{Y}}$. 
We again make the (admittedly unrealistic) assumption that the random influences $U_{\color{green}{X}}, U_{\color{violet}{Z}}, U_{\color{blue}{Y}}$ are independent of each other.


```{r kind, echo=FALSE}
question("In the language of causal graphs: Is understanding ($Y$) a child of learning ($X$)?",
         answer("Yes"),
         answer("No", correct = TRUE, message = "Understanding is a child of knowledge ($Z$). Knowledge is in turn a child of learning. Thus, understanding is a descendant, but not directly a child of learning. Understanding *listens* directly only to knowledge, i.e., the value of understanding depends directly only on knowledge."),
         allow_retry = TRUE,
         correct = "Great, Correct!",
         incorrect = "Unfortunately, this was wrong. Hint: Grandchildren are not the same as children!")
```

## Mediators

In such scenarios
$$\color{green}{X} \rightarrow \color{violet}{Z} \rightarrow \color{blue}{Y}$$
the variable in the middle &ndash; here $\color{violet}{Z}$ &ndash; is called a **mediator**.

```{r mediator, echo=FALSE}
question("Suppose whether a candidate is promoted depends on their gender due to discrimination, and the future salary of course depends on the promotion. Which variable is a mediator here?",
         answer("Gender"),
         answer("Promotion", correct = TRUE, message = "The causal model described is $\\text{Gender} \\rightarrow \\text{Promotion} \\rightarrow \\text{Salary}$."),
         answer("Salary"),
         allow_retry = TRUE,
         correct = random_praise(),
         incorrect = random_encouragement()
         )
```

To study the causal effect of the cause ($\color{green}{X}$) on the effect ($\color{blue}{Y}$), we do not need to know the value of the mediator ($\color{violet}{Z}$). So, for example, if we want to know how <green>gender</green> affects <blue>salary</blue> overall, we do not need information about <violet>promotions</violet>.

In fact, accounting for the mediator can even cause the causal effect to be biased. 
We will look at this in more detail below with the help of a small simulation. 

***

*Note*: In the context of mediation, we can distinguish between *total*, *direct*, and *indirect* effects. 
In the example, we are interested in the so-called total effect of gender: How does gender affect salary, regardless of the specific mechanisms involved?

If instead we were interested in the extent to which gender has an effect on salary independent of promotion, the target of analysis would be the so-called direct effect. 
And if we were interested in the extent to which the effects of gender are mediated by promotion (do women earn less because they are less likely to be promoted?), the analysis target would be the indirect effect.

***


## Data simulation

Let us return to the associations between learning, knowledge and understanding.
We simulate the following in `R`:

\begin{eqnarray*}
\color{green}{X} &=& U_{\color{green}{X}}\\
\color{violet}{Z} &=& f_{\color{violet}{Z}}(\color{green}{X}, U_{\color{violet}{Z}})\\
\color{blue}{Y} &=& f_{\color{blue}{Y}}(\color{violet}{Z},U_{\color{blue}{Y}}).
\end{eqnarray*}

Recall from Module 2 that $U$ denotes unknown causes and $f$ the functions by which values are assigned to the variables.

Simulate observations by clicking 'Run' several times and try to see how the variables are related to each other:

```{r sim, exercise=TRUE}
x <- U_X()
cat("Value of x (learning):", x,"\n")
z <- f_Z(x)
cat("Value of z (knowledge):", z,"\n")
y <- f_Y(z)
cat("Value of y (understanding):", y,"\n")
```

```{r beobachtung, echo=FALSE}
question("What happens when you observe higher values of learning (`x`)?",
         answer("With higher values of learning (`x`), higher values of understanding (`y`) usually occur.", correct = TRUE, message = "A positive correlation between $X$ and $Y$ can be observed."),
         answer("Higher values of learning (`x`) tend to have lower values of understanding (`y`)."),
         answer("The value of understanding (`y`) does not appear to be related to the value of learning (`x`)."),
         allow_retry = TRUE,
         correct = random_praise(),
         incorrect = random_encouragement()
         )
```

## Intervention

Instead of just observing ($\color{green}{X} = U_{\color{green}{X}}$), we can also simulate an intervention where we set values ($do(\color{green}{X}=x)$).

In the code example: $do(\color{green}{X}=1)$. 
Press `Run Code` a few times to see what the values of understanding ($\color{blue}{Y}$) look like in the case of $do(\color{green}{X}=1)$. 
Then change the code to simulate $do(\color{green}{X}=10)$. 
What happens? 

```{r simdo, exercise=TRUE}
 # Here is the comman to assign do(X=1)
x <- 1
cat("Value of x (learning):", x, "\n")
z <- f_Z(x)
cat("Value of z (knowledge):", z, "\n")
y <- f_Y(z)
cat("Value of y (understanding):", y, "\n")
```

```{r simdo-solution}
 # do(X=10)
x <- 10
cat("Value of x (learning):", x, "\n")
z <- f_Z(x)
cat("Value of z (knowledge):", z, "\n")
y <- f_Y(z)
cat("Value of y (understanding):", y, "\n")
```

What happens here?

##

While the values of $\color{blue}{Y}$ fluctuate around $\color{blue}{15}$ for $do(\color{green}{X}=1)$, they fluctuate around $\color{blue}{150}$ for $do(\color{green}{X}=10)$. 
Thus, we see that a change in $\color{green}{X}$ actually leads to a change in $\color{blue}{Y}$. 
This causal relationship is mediated by $\color{violet}{knowledge}$: More learning leads to more knowledge leads to more understanding.


```{r intervention, echo=FALSE}
question("Consider: What will happen to the relationship between learning (`x`) and understanding (`y`) when we know that knowledge takes on a certain value, e.g., `z <- 15`?",
         answer("As a rule, for higher values of learning (`x`), higher values of understanding (`y`) continue to occur.",),
         answer("For higher values of learning (`x`), now lower values of understanding (`y`) generally occur.",),
         answer("For fixed knowledge (`z`), the value of understanding (`y`) is unrelated to the value of learning (`x`).", correct = TRUE, message = "Knowledge of `z` breaks the causal chain from $X$ to $Y$. We had said before that understanding only listens  to knowledge. So if knowledge does not change, understanding does not change here either. You can test this in the next simulation."),
         allow_retry = TRUE,
         correct = random_praise(),
         incorrect = random_encouragement()
         )
```

##

Feel free to try this out by pressing `Run Code`:

```{r sim2, exercise=TRUE}
x <- U_X()
cat("Value of x (learning):", x,"\n")
z <- 15
cat("Value of z (knowledge):", z,"\n")
y <- f_Y(z)
cat("Value of y (understanding):", y,"\n")
```

The  value of `z` is set to $15$ in line 3, regardless of the value of `x`.
Now `x` and `y` still fluctuate randomly, but are independent of each other.

## Causal model

The underlying equations of the causal model that we just simulated are:

\begin{eqnarray*}
\color{green}{X} &=& U_{\color{green}{X}}, \quad U_{\color{green}{X}} \sim \mathcal{G}(1,\,10), \\
\color{violet}{Z} &=& 5 \cdot \color{green}{X} +  U_{\color{violet}{Z}}, \quad U_{\color{violet}{Z}} \sim \mathcal{N}(0,\,1), \\
\color{blue}{Y} &=& 3 \cdot \color{violet}{Z} + U_{\color{blue}{Y}}, \quad U_{\color{blue}{Y}} \sim \mathcal{N}(0,\,1).
\end{eqnarray*}

Here $\mathcal{G}(1,\,10)$ stands for the *uniform distribution* on the range $1$ to $10$ and $\mathcal{N}(0,\,1)$ for a *normal distribution* with the parameters $\mu=0$ and $\sigma=1$, i.e., a standard normal distribution. 
The functions and parameters were arbitrarily chosen.

Substituting $f_{\color{violet}{Z}}$ into $f_{\color{blue}{Y}}$ results in 
$\color{blue}{Y} = 3 \cdot (5 \cdot \color{green}{X} + U_{\color{violet}{Z}}) + U_{\color{blue}{Y}}=15 \cdot \color{green}{X} + 5 \cdot U_{\color{violet}{Z}} + U_{\color{blue}{Y}}.$

For $n=100$ simulated observations, the corresponding `R` code is:

```{r RSim, eval = FALSE}
## Preparation 
library(mosaic) # load package
set.seed(1896)  # fix random number generator to ensure reproducibility

## functions
U_X <- function(n = 1) runif(n, min = 1, max = 10)
f_Z <- function(x) 5 * x + rnorm(length(x))
f_Y <- function(z) 3 * z + rnorm(length(z))

## data table
n <- 100 # number of observations
SimData <- tibble(x = U_X(n)) %>%
  mutate(z = f_Z(x)) %>%
  mutate(y = f_Y(z)
```

## Linear regression, attempt 1

Of course, in most cases, we don't know which system of equations underlies our data. 
Instead, we collect data and then examine the associations to make inferences about the underlying system.
One method for estimating relationships between variables $\color{green}{X}$ and $\color{blue}{Y}$ using observed data is **linear regression**.

***

*Note*: You can learn more about linear regression in the AI Campus course [The Elements of AI](https://ki-campus.org/courses/elementsofai), chapter on machine learning.

***

Here, we assume that the relationship between the variable to be explained $\color{blue}{Y}$ and the other variables in the model is linear, i.e., it is sufficient to only estimate the respective slope to describe the relationship.
This is what it looks like when we calculate the relationship between learning and understanding in our simulated data:

```{r streu, out.width='80%', fig.align='center', echo = FALSE, warning=FALSE}
gf_point(y ~ x, data = SimData) %>% # scatter plot
  gf_lm() %>% # regression line
  gf_labs(x = "x: learning", y = "y: understanding") # label axes 
```

In `R` a linear regression can be run via the function `lm()`.

Without the mediator <violet>knowledge</violet>, we get the following model:

```{r lmoz}
# Calculate regression
ModelA <- lm(y ~ x, data = SimData)
# Results
ModelA
```

So our model looks like this:

$$\widehat{\color{blue}{\text{Understanding}}} = `r round(coef(ModelA)[1],2)` + `r round(coef(ModelA)[2],2)` \times \color{green}{\text{Learning}}$$

According to this model, the (total) causal effect of <green>learning</green> on <blue>understanding</blue> is $`r round(coef(ModelA)[2],2)`$: If <green>learning</green> is increased by one unit, the mean of <blue>understanding</blue> increases by $`r round(coef(ModelA)[2],2)`$ units.

This is consistent with the results of our simulated intervention: while the values for $\color{blue}{Y}$ fluctuated around $\color{blue}{15}$ for $do(\color{green}{X}=1)$, they were around $\color{blue}{150}$ for $do(\color{green}{X}=10)$. 
The linear regression results are thus indeed consistent with the causal effect of interest.


***

*Note*: This course focuses on the identification of causal effects.
We will thus spend little time on estimation procedures and statistical inference.
However, if you are already familiar with linear regression, here is how to get the *usual* regression table incl. standard errors, p values, etc. via `summary()`:

```{r summary, exercise = TRUE}
# Run regression
ModelA <- lm(y ~ x, data = SimData)
# Results
summary(ModelA)
```

***

## Linear regression, attempt 2

But what happens if the mediator <violet> knowledge</violet> ($\color{violet}{Z}$) is included in the model?
Now the results change:

```{r lmmz}
# Run regression
ModelB <- lm(y ~ x + z, data = SimData)
# Results
ModelB
```

Returning the following model:

$$\widehat{\color{blue}{\text{Understanding}}} = `r round(coef(ModelB)[1],2)` + `r round(coef(ModelB)[2],2)` \times \color{green}{\text{Learning}} + `r round(coef(ModelB)[3],2)` \times \color{violet}{\text{Knowledge}}$$
If <violet>knowledge</violet> is part of the model while we try to determine the causal effect of <green>learning</green> on <blue>understanding</blue>, our model now says: If <green>learning</green> is increased by one unit, the mean value of <blue>understanding </blue> increases by $`r round(coef(ModelB)[2],2)`$ units &ndash; a much smaller value than before ($`r round(coef(ModelA)[2],2)`$).

```{r adjustierung, echo=FALSE}
question("Which value correctly describes the (total) causal effect of learning (`x`) on understanding (`y`)? That is, by how many units will the value of understanding change on average if one unit more is learned?",
         answer("The value from the model without knowledge (`modelA`), i.e. $14.86$.", correct = TRUE, message = "As we had observed in the simulated intervention, this is the correct value. The model which additionally includes the mediator breaks the causal chain from $X$ to $Y$."),
         answer("The value from the model with knowledge (`ModelB`), i.e. $0.86$."),
         allow_retry = TRUE,
         correct = random_praise(),
         incorrect = random_encouragement()
         )
```


***

*Note*: The estimated values in the regression do not correspond to the true values we used for the simulation due to random noise.

***

## Summary

:::{.box}
To determine the (total) causal effect of $X$ on $Y$ in a chain 
$$X \rightarrow Z \rightarrow Y$$
any mediator $Z$ should **not** be considered. When $Z$ is conditioned upon (e.g., when the variable is included in a regression), the causal relationship between $X$ and $Y$ is broken.
::: 

## AI Campus

[Back to course](https://learn.ki-campus.org/courses/b4f3bf71-74b1-48d8-a4c4-61aef5a808eb/launch)