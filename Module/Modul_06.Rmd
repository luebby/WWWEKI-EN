---
title: "Module 06: Kind or handsome? Why not both?"
output: 
  learnr::tutorial:
    progressive: true
    css: "css/style.css"
runtime: shiny_prerendered
---

<a href="https://ki-campus.org/">
<img border="0" alt="KICampusLogo" src="images/KIcampusLogo.png" width="100" height="30" style="float: right">
</a>

```{r setup, include=FALSE}
library(learnr)
library(knitr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(emojifont)
library(ggthemes)

theme.fom <- theme_classic(22*1.04)
theme.fom <- theme.fom
theme_set(
  theme.fom  
)

library(ggdag)
# DAG
co <- data.frame(x=c(2,1,0), y=c(1,0,1), name=c("Y","Z","X"))

DAG_Collider <- dagify(Z ~ Y,
                  Z ~ X, coords = co) %>% 
  ggdag() +
  geom_dag_point(colour = c("#0F710B","#0000FF", "#DA70D6")) + 
  geom_dag_text(size = 8) +
  theme_dag_blank() +
  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(15, "pt"), type = "closed"))  + 
  geom_text(label = "Y - Looks\nX - Kindness\nZ - Date",
            hjust = 0.5, vjust = 1,
            x = 1, y = 1, size = 7, color = "darkgrey")


library(mosaic)

# Daten und Funktion
set.seed(1896)
n <- 100

SimData <- tibble(x = rnorm(n), y = rnorm(n), u_z = rbinom(n, size = 1, prob = 0.05)) %>%
  mutate(z = (x > 1) | (y > 1)) %>%
  mutate(z = (1-u_z) * z + u_z * (1-z)) %>%
  mutate(z = ifelse(z, "Yes", "No"))
```

## Learning objectives

In this module you will learn:

- about inverted forks,

- colliders, and

- that we sometimes unintentionally create associations where there are none.

## All good things come in threes

Even complex causal diagrams consist of relatively simple basic elements. Besides the chain and the fork, there is the **inverted fork**. 

Remember: $$A \rightarrow B$$ says that $B$ *listens* to $A$, but not vice versa. 

## Dating

Again, highly simplified: Let's assume that kidness and looks play a role in whether we go on a date with someone.

Would you date someone who was neither kind nor good-looking? Maybe, but more likely not.


Suppose <green>kindness</green> ($\color{green}{X}$) leads to <violet>date</violet> ($\color{violet}{Z}$). Also, (good) <blue>looks</blue> ($\color{blue}{Y}$) lead to <violet>date</violet> ($\color{violet}{Z}$). 

So you'd date someone who is nice *and/or* good looking.

This assumed model can be represented with the following causal graph:

```{r DAG_Collider, echo=FALSE, fig.align='center', out.width='85%'}
plot(DAG_Collider)
```

##

The structural causal model consists of the following assignments:

\begin{eqnarray*}
\color{green}{X} &=& U_{\color{green}{X}}\\
\color{blue}{Y} &=& U_{\color{blue}{Y}}\\
\color{violet}{Z} &=& f_{\color{violet}{Z}}(\color{green}{X}, \color{blue}{Y}, U_{\color{violet}{Z}})
\end{eqnarray*}


```{r abhaengigkleit, echo=FALSE}
message <- "By construction, $X$ and $Y$ are independent. No causal path leads from $X$ to $Y$ &ndash; or vice versa."
question("Do looks ($Y$) depend on kindness ($X$) in this example?",
         answer("Yes"),
         answer("No", correct = TRUE, message = message),
         allow_retry = TRUE,
         correct = random_praise(),
         incorrect = random_encouragement()
         )
```

## Colliders

In cases like these: 
$$\color{green}{X} \rightarrow \color{violet}{Z} \leftarrow \color{blue}{Y}$$
the variable in the middle &ndash; here  $\color{violet}{Z}$ &ndash; is called a **collider**. $\color{violet}{Z}$ is an effect of $\color{green}{X}$ and $\color{blue}{Y}$.

```{r collider, echo=FALSE}
message <- "If it wasn't luck or bad luck ($U_Z$), then the unkind person has to look rather good. After all, there is a reason you dated them in the first place. If it wasn't kindness, it was probably looks."
question("Suppose you dated someone ($Z$) who is not particularly kind ($X$). Does this tell you anything about their looks ($Y$)?",
         answer("Yes", correct = TRUE, message = message),
         answer("No"),
         allow_retry = TRUE,
         correct = random_praise(),
         incorrect = random_encouragement()
         )
```

## Contrasting fork and inverted fork

Recall that in a **fork**, there is a non-causal path from $\color{green}{X}$ to $\color{blue}{Y}$:
$$\color{green}{X} \leftarrow \color{violet}{Z} \rightarrow \color{blue}{Y}$$

Suppose: In summer, <violet>sunshine</violet> leads to an increase in both <green>ice cream</green> and <blue>sunscreen</blue> sales.
<green>Ice cream</green> and <blue>sunscreen</blue> are not independent: If I know that a lot of <green>ice cream</green> was sold, I can assume that a lot of <blue>sunscreen</blue> was sold as well. 
Presumably, <violet>sunshine</violet> led to high <green>ice cream </green> sales and also to high <blue>sunscreen</blue> sales. The relationship is observed when no other variables are taken into account -- it is also called *unconditional,* *marginal*.

<green>Ice cream</green> tells us something about <violet>sunshine</violet> which tells us something about <blue>sunscreen</blue>.
But if I know that there is <violet>sunshine</violet>, the information that a lot of <green>ice cream</green> has been sold contains no additional information about <blue>sunscreen</blue> sales. 
*Conditional* on <violet>sunshine</violet>, that is, if I know that the sun is shining, <green>ice cream</green> and <blue>sunscreen</blue> sales are independent.

To summarize: In a fork, there is an unconditional association between $\color{green}{X}$ and $\color{blue}{Y}$; but given $\color{violet}{Z}$ -- that is, conditional -- there is no relationship.

In an **inverted fork** there is no association between $\color{green}{X}$ and $\color{blue}{Y}$:
$$\color{green}{X} \rightarrow \color{violet}{Z} \leftarrow \color{blue}{Y}$$
Both on the <green>weekend</green> and on <blue>vacation</blue>, people can <violet>sleep in</violet>.
Let's assume that weekends and vacation are uncorrelated.
<green>Weekend</green> and <blue>vacation</blue> will be (*unconditionally*, *marginally*) independent: 
I learn nothing about <blue>vacation</blue> from the information <green>weekend</green>.
But if I know that I can <violet>sleep in</violet>, I know that it is either <green>weekend</green> or <blue>vacation</blue> (or both).
Conditional on <violet>sleeping in</violet>, I learn from the information that there was no <blue>vacation</blue>, that it is probably <green>weekend</green>.
There must be a reason for my sleeping in.
*Conditionally*, given <violet>sleeping in</violet>, <green>weekend</green> and <blue>vacation</blue> are thus no longer independent.

The inverted fork thus behaves exactly the other way around than the normal fork.
There is no unconditional association, but there is a conditional association.


## Model and data simulation

Consider the following structural causal model:

\begin{eqnarray*}
\color{green}{X} &=& U_{\color{green}{X}}, \quad U_{\color{green}{X}} \sim \mathcal{N}(0,\,1), \\
\color{blue}{Y} &=& U_{\color{blue}{Y}}, \quad U_{\color{blue}{Y}} \sim \mathcal{N}(0,\,1), \\
\tilde{\color{violet}{Z}} &=&\begin{cases} 1 &  \text{if } \{ \color{green}{X} > 1 \,\vee\, \color{blue}{Y} > 1\} \\ 0 &  \text{otherwise} \end{cases}, \\
\color{violet}{Z} &=& (1-U_{\color{violet}{Z}}) \cdot \tilde{\color{violet}{Z}} + U_{\color{violet}{Z}} \cdot (1- \tilde{\color{violet}{Z}}), \quad U_{\color{violet}{Z}} \sim \mathcal{B}(0.05),
\end{eqnarray*}

$\mathcal{N}(0,\,1)$ is a standard normal distribution, $\mathcal{B}(0.05)$ is a Bernoulli distribution with $\pi=0.05$. $\tilde{\color{violet}{Z}}$ is an auxiliary variable that takes the value $1$ if $\color{green}{X}$ or $\color{blue}{Y}$ are greater than $1$. Otherwise, $\tilde{\color{violet}{Z}}=0$. Whether $\color{violet}{Z}$ is really $0$ (no date) or $1$ (date) partially depends on random luck.

$\vee$ is the logical *or* operator (`|` in `R`). 



The following `R` code simulates this data generating mechanism:

```{r sim, eval=FALSE}
library(mosaic) # load package
set.seed(1896)  # set random seed for reproducibility
n <- 100        # number of observations

SimData <- tibble(x = rnorm(n),                               # X 
                  y = rnorm(n),                               # Y
                  u_z = rbinom(n, size = 1, prob = 0.05)) %>% # U_z
  mutate(z = (x > 1) | (y > 1)) %>%                           # Z~
  mutate(z = (1-u_z) * z + u_z * (1-z)) %>%                   # Z
  mutate(z = ifelse(z, "Yes", "No"))
```

Both the mathematical representation and the `R` code are a bit more complex here. 
But what matters here is the data that comes out of it:

```{r scatter, echo=FALSE, fig.align='center', out.width='85%'}
gf_point(y ~ x, data = SimData, color = ~z) +
  scale_color_colorblind()  +
  xlab("x (kindness)") +
  ylab("y (looks)")
```

## Associations

These data describe the previously described dating situation. 
The color of the dots tells us if we have dated someone ($\color{violet}{Z}$). 
$\color{green}{X}$ is kindness and $\color{blue}{Y}$ is looks.

If, separately for the two levels of <violet>date</violet> ($\color{violet}{Z}$), we run a linear regression of <blue>looks</blue> ($\color{blue}{Y}$) on <green>kindness</green> ($\color{green}{X}$), the results look like this:

```{r scatterlm, echo=FALSE, fig.align='center', out.width='85%'}
gf_point(y ~ x, data = SimData, color = ~z) %>%
  gf_lm() +
  scale_color_colorblind() +
  xlab("X (kindness)") +
  ylab("Y (looks)")
```

```{r corb, echo=FALSE}
message <- "The regression line goes from top left to bottom right. This indicates a negative correlation. Those dates who are particularly good looking are not particularly kind &ndash; and vice versa."
question("For those you have dated (`Z = Yes`): Do you see a correlation between kindness `X` and looks `Y`?", 
         answer("Yes", correct = TRUE, message = message),
         answer("No"),
         allow_retry = TRUE,
         correct = random_praise(),
         incorrect = random_encouragement()
         )
```

##

Thus, when we consider <violet>date</violet> ($\color{violet}{Z}$), we see a relationship between <blue>looks</blue> ($\color{blue}{Y}$) and <green>kindness</green> ($\color{green}{X}$). 
But originally, we simulated our data so that the two variables are independent of each other.

If we look at the relationship between $\color{green}{X}$ and $\color{blue}{Y}$ without considering $\color{violet}{Z}$, we can see that the variables are indeed independent:

```{r scatterlmub, echo=FALSE, fig.align='center', out.width='85%'}
gf_point(y ~ x, data = SimData) %>%
  gf_lm()
```

***

***Note:*** The fact that the straight line here is not completely parallel to the x-axis is due to random variation. Even if there is no correlation between the variables in the data generating process ($\rho=0$), there may be one in a (simulated) sample ($r\neq0$).

***

We often hear the sentence:

> Don't trust any statistic that you haven't falsified yourself

If we take our dates as a sample for the analysis of a possible association between looks and kindness, we should better say:

> Don't trust any statistic that you have falsified yourself.

Of course our dates are not *fake*, but we have a self-selected sample on which we base our analyses.
This delivers a distorted result with correlations in places where there are actually none.

## Summary

:::{.box}
To determine the causal effect of $X$ on $Y$ in an inverted fork 
$$X \rightarrow Z \leftarrow Y$$
the collider $Z$ must not be considered. (This is also true for all descendants of $Z$).
If $Z$ is conditioned upon, a spurious association between $X$ and $Y$ is generated and biases the analysis. 
For example, one should not include $Z$ as a predictor variable in a linear model.
Nor should one divide the data into groups based on $Z$ and analyze them in separation -- this also distorts associations.
::: 

## Note

This course was supported by a grant from the German Federal Ministry of Education and Research, grant number 16DHBQP040.


![](images/csm_Logo-BMBF.jpg)


